{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_boats.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1T704nvB2Ecz4rA5JbKqCxunY6dTWLoNh",
      "authorship_tag": "ABX9TyOXJuvFuunyZjkQZ6Zmplsv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirovix/CV_final_project/blob/main/NN_boats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFtjNmc4faB5"
      },
      "source": [
        "import json\n",
        "import io\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6PawTwU0m99"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/MyDrive/dataset_boats/label.json') as json_file:\n",
        "    json_file = json.load(json_file)\n",
        "\n",
        "#print(len(json_file[30]['Label']))\n",
        "\n",
        "x_start, y_start, x_end, y_end = [],[],[],[]\n",
        "data,targets = [],[]\n",
        "width = 32\n",
        "height = 32\n",
        "\n",
        "for row in json_file:\n",
        "\n",
        "  if(len(row['Label']) == 0):\n",
        "    continue\n",
        "\n",
        "  x_start = float(row['Label']['objects'][0]['bbox']['left'])\n",
        "  y_start = float(row['Label']['objects'][0]['bbox']['top'])\n",
        "  x_end = float(row['Label']['objects'][0]['bbox']['left'] + row['Label']['objects'][0]['bbox']['width'])\n",
        "  y_end = float(row['Label']['objects'][0]['bbox']['top'] + row['Label']['objects'][0]['bbox']['height'])\n",
        "  \n",
        "  image_path = \"/content/drive/MyDrive/dataset_boats/train/\" + row['External ID']\n",
        "  cv_image = cv2.imread(image_path)\n",
        "\n",
        "  x_start = (width * x_start) / cv_image.shape[1]\n",
        "  y_start = (height * y_start) / cv_image.shape[0]\n",
        "  x_end = (width * x_end) / cv_image.shape[1]\n",
        "  y_end = (height * y_end) / cv_image.shape[0]\n",
        "  targets.append((x_start, y_start, x_end, y_end))\n",
        "\n",
        "  cv_image = cv2.resize(cv_image, (height, width))\n",
        "  data.append(img_to_array(cv_image))\n",
        "  \n",
        "\n",
        "\n",
        "print(len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HUTs1SzS2sO"
      },
      "source": [
        "print(cv_image.shape[0])\n",
        "print(cv_image.shape[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGrgVOV15LPr"
      },
      "source": [
        "# convert the data and targets to NumPy arrays, scaling the input\n",
        "# pixel intensities from the range [0, 255] to [0, 1]\n",
        "#data = np.array(data, dtype=\"float32\") / 255.0\n",
        "#targets = np.array(targets, dtype=\"float32\")\n",
        "# partition the data into training and testing splits using 90% of\n",
        "# the data for training and the remaining 10% for testing\n",
        "data = np.array(data, dtype=\"float32\") / 255\n",
        "targets = np.array(targets, dtype=\"float32\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.15, random_state=42)\n",
        "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.47, random_state=42) # 0.25 x (1 - 0.15) = 0.2125"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snI7umSdW1Dx"
      },
      "source": [
        "mean = np.mean(X_train, axis = 0)\n",
        "std = np.std(X_train, axis = 0)\n",
        "\n",
        "X_train = (X_train - mean) /  std\n",
        "X_val = (X_val - mean) /  std\n",
        "X_test = (X_test - mean) /  std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bdk2go053ao"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.plot(history.epoch,history.history['loss'])\n",
        "  plt.plot(history.epoch,history.history['val_loss'])\n",
        "  plt.title('loss')\n",
        "\n",
        "def plot_accuracy(history):\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.plot(history.epoch,history.history['accuracy'])\n",
        "  plt.plot(history.epoch,history.history['val_accuracy'])\n",
        "  plt.title('accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA46MLaz56JY"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters=128, kernel_size=[3,3], activation='relu', input_shape=[height, width, 3]),\n",
        "    keras.layers.MaxPool2D(pool_size=[2,2]),\n",
        "    keras.layers.Conv2D(filters=64, kernel_size=[2,2], activation='relu'),\n",
        "    keras.layers.MaxPool2D(pool_size=[2,2]),\n",
        "    keras.layers.Conv2D(filters=32, kernel_size=[2,2], activation='relu'),\n",
        "    keras.layers.MaxPool2D(pool_size=[2,2]),\n",
        "    keras.layers.Conv2D(filters=16, kernel_size=[2,2], activation='relu'),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(4, activation=\"sigmoid\")\n",
        "  ])\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "  \n",
        "print(model.summary())\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=2 ,validation_data=(X_val, y_val))\n",
        "\n",
        "plot_loss(history)\n",
        "plot_accuracy(history)\n",
        "\n",
        "scores = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(\" %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "print(\"----------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}